{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHK6DyunSbs4"
   },
   "source": [
    "## Training Study: Assay Model Architecture Exploration\n",
    "\n",
    "This script performs model training experiments to study the effect of convolutional layer depth and dense neuron count using specified assay datasets.\n",
    "It includes dataset preparation and systematic testing of different architectural configurations, producing metrics for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.data_training import *  # Custom module for training models\n",
    "from activity_constants import *           # Constants such as paths, filenames, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Clears the current Keras backend session to avoid clutter from old models.\n",
    "########################################################################\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# get_dataset_layer_study: Constructs model names and output paths based \n",
    "# on input parameters, and triggers the training process using those settings.\n",
    "#\n",
    "# Inputs:\n",
    "#   - p_assay: List of assay IDs.\n",
    "#   - p_output_subfolder: Subfolder for saving results.\n",
    "#   - dataset: Tuple of train/validation/test data.\n",
    "#   - p_epocs: Number of training epochs.\n",
    "#   - p_conv_extr: List of convolutional layers [filters, kernel_size].\n",
    "#   - p_dense: List of dense layers [units, dropout].\n",
    "#   - p_metrics: Evaluation metrics (default: acc, precision, recall, f1).\n",
    "########################################################################\n",
    "def get_dataset_layer_study(p_assay, p_output_subfolder, dataset, p_epocs, p_conv_extr, p_dense#, p_metrics = ['acc']):\n",
    "                              , p_metrics = ['acc'\n",
    "                               , precision\n",
    "                               , recall\n",
    "                               , f1\n",
    "                                           ] \n",
    "                           ):\n",
    "    \n",
    "    assay_item_name = 'model_'\n",
    "    assay_codes = ''\n",
    "    for assay in p_assay:\n",
    "        if (len(assay_codes) > 0):\n",
    "            assay_codes = assay_codes + '#'\n",
    "        assay_codes = assay_codes + (str)(assay)\n",
    "        \n",
    "    assay_item_name =  assay_item_name + assay_codes    \n",
    "    for conv_extr in p_conv_extr:\n",
    "      assay_item_name =  assay_item_name + '#{0}_{1}'.format(conv_extr[0], conv_extr[1])\n",
    "        \n",
    "    assay_item_name =  assay_item_name + '#'\n",
    "    for dense in p_dense:\n",
    "      assay_item_name =  assay_item_name + '#{0}_{1}'.format(dense[0], dense[1])\n",
    "    \n",
    "    print ('Creating model {0}'.format(assay_item_name))\n",
    "    \n",
    "    output_folder = model_folder\n",
    "    \n",
    "    if (len(p_output_subfolder)>0):\n",
    "        output_folder = output_folder + '/' + p_output_subfolder\n",
    "        \n",
    "    print ('Output folder {0}'.format(output_folder))\n",
    "    \n",
    "    print ('''train_assay_model({}\n",
    "                    , {}\n",
    "                    , {}\n",
    "                    , {}\n",
    "                    , {}\n",
    "                    , {}\n",
    "                    , {}\n",
    "                    , {}\n",
    "                    , {}\n",
    "                     )'''.format(data_set_folder\n",
    "                    , p_assay\n",
    "                    , output_folder\n",
    "                    , 1\n",
    "                    ,  p_epocs\n",
    "                    , p_metrics\n",
    "                    , assay_item_name\n",
    "                    , p_conv_extr\n",
    "                    , p_dense))\n",
    "    \n",
    "    train_assay_model(data_set_folder\n",
    "                    , p_assay\n",
    "                    , output_folder\n",
    "                    , dataset\n",
    "                    , 1\n",
    "                    , p_epochs = p_epocs\n",
    "                    , p_metrix_list=p_metrics\n",
    "                    , p_assay_item_name = assay_item_name\n",
    "                    , p_conv_extr = p_conv_extr\n",
    "                    , p_dense = p_dense                  \n",
    "                     )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# get_dataset_layer_study_standard: Automates training over multiple\n",
    "# convolutional depths with fixed dense layers.\n",
    "#\n",
    "# Inputs:\n",
    "#   - assay: Assay ID.\n",
    "#   - p_output_subfolder: Subfolder name for output.\n",
    "#   - dataset: Training/validation/test data.\n",
    "#   - p_epocs: Number of training epochs.\n",
    "#   - p_filter_size: Kernel size for conv layers.\n",
    "########################################################################\n",
    "def get_dataset_layer_study_standard(assay, p_output_subfolder, dataset, p_epocs , p_filter_size):\n",
    "    get_dataset_layer_study(assay, p_output_subfolder, dataset, p_epocs, \n",
    "                            p_conv_extr=[[16, p_filter_size]], p_dense=[[128, 0]])\n",
    "    get_dataset_layer_study(assay, p_output_subfolder, dataset, p_epocs, \n",
    "                            p_conv_extr=[[16, p_filter_size], [32, p_filter_size]], p_dense=[[128, 0]])\n",
    "    get_dataset_layer_study(assay, p_output_subfolder, dataset, p_epocs, \n",
    "                            p_conv_extr=[[16, p_filter_size], [32, p_filter_size], [64, p_filter_size]], p_dense=[[128, 0]])\n",
    "    get_dataset_layer_study(assay, p_output_subfolder, dataset, p_epocs, \n",
    "                            p_conv_extr=[[16, p_filter_size], [32, p_filter_size], [64, p_filter_size], [128, p_filter_size]], p_dense=[[128, 0]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Dataset loading and preparation.\n",
    "########################################################################\n",
    "dataset_folder = get_assay_folder_dataset_name(data_set_folder, prefix_dataset_folder, [1806])\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = get_data_from_files (dataset_folder, dataRegionEspFromX, dataRegionEspFromY, dataRegionEspNumColumns, dataRegionEspNumRows, 1, None)\n",
    "dataset = [x_train, y_train, x_val, y_val, x_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Run convolutional layer depth study (study_layer).\n",
    "########################################################################\n",
    "study_name = 'study_layer'\n",
    "global_overview_folder = model_folder + '/' + study_name\n",
    "create_global_overview_file (global_overview_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests for various filter sizes\n",
    "for filter_size in [3, 5, 7, 11, 13, 15]:\n",
    "    get_dataset_layer_study_standard([1806], study_name, dataset, default_data_training_num_epocs, filter_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# get_dataset_layer_study_dense: Automates dense layer variations \n",
    "# while keeping convolutional structure fixed.\n",
    "#\n",
    "# Inputs:\n",
    "#   - assay: Assay ID.\n",
    "#   - p_output_subfolder: Output folder name.\n",
    "#   - dataset: Full dataset.\n",
    "#   - p_epocs: Number of training epochs.\n",
    "#   - p_neurons: Number of neurons in dense layers.\n",
    "########################################################################\n",
    "def get_dataset_layer_study_dense(assay, p_output_subfolder, dataset, p_epocs, p_neurons):\n",
    "    get_dataset_layer_study(assay, p_output_subfolder, dataset, p_epocs, \n",
    "                            p_conv_extr=[[16, 7], [32, 7]],\n",
    "                            p_dense=[[p_neurons, 0], [p_neurons, 0]])\n",
    "    get_dataset_layer_study(assay, p_output_subfolder, dataset, p_epocs, \n",
    "                            p_conv_extr=[[16, 7], [32, 7]],\n",
    "                            p_dense=[[p_neurons, 0], [p_neurons, 0], [p_neurons, 0]])\n",
    "    get_dataset_layer_study(assay, p_output_subfolder, dataset, p_epocs, \n",
    "                            p_conv_extr=[[16, 7], [32, 7]],\n",
    "                            p_dense=[[p_neurons, 0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Run dense layer neuron count study (study_dense).\n",
    "########################################################################\n",
    "study_name = 'study_dense'\n",
    "global_overview_folder = model_folder + '/' + study_name\n",
    "create_global_overview_file(global_overview_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests for different neuron counts\n",
    "for neurons in [16, 32, 64, 128, 256, 512]:\n",
    "    get_dataset_layer_study_dense([1806], study_name, dataset, default_data_training_num_epocs, neurons)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of image_classification_part2.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part2.ipynb",
     "timestamp": 1590165853129
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
