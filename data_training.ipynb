{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YHK6DyunSbs4"
   },
   "source": [
    "## Common functions for CNN model training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 16:36:30.439139: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-01 16:36:31.035895: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-01 16:36:32.060756: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/oracle/11.2/client64/lib\n",
      "2025-06-01 16:36:32.060830: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/oracle/11.2/client64/lib\n",
      "2025-06-01 16:36:32.060838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Module and library imports\n",
    "########################################################################\n",
    "\n",
    "# Import custom activity constants\n",
    "from activity_constants import *\n",
    "\n",
    "\n",
    "# Garbage collector for memory management\n",
    "import gc\n",
    "\n",
    "# Operating system library for file handling\n",
    "import os\n",
    "\n",
    "# Numerical computing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Image processing utilities from TensorFlow/Keras\n",
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "# Data preprocessing tools from scikit-learn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# TensorFlow and Keras libraries for building neural networks\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Utilities for date and time handling\n",
    "from datetime import datetime\n",
    "\n",
    "# Metrics computation tools from scikit-learn\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Defining subset region\n",
    "########################################################################\n",
    "\n",
    "# Define window width\n",
    "dataRegionEspNumColumns = 1350\n",
    "\n",
    "# Define window height\n",
    "dataRegionEspNumRows = 295\n",
    "\n",
    "dataRegionEspFromX = 0\n",
    "dataRegionEspFromY = 0\n",
    "\n",
    "\n",
    "default_data_training_num_epocs = 5\n",
    "default_positive_perc_th = 0.9\n",
    "\n",
    "default_batch_size = 15\n",
    "\n",
    "# Incidamos que se use cache de lectura de datos\n",
    "use_cache = False\n",
    "file_cache = {}\n",
    "\n",
    "default_global_overview_file_name = \"global_overview.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# print_trace: Prints a timestamped log message.\n",
    "#\n",
    "# Inputs:\n",
    "#   - trace: Message string to print.\n",
    "#\n",
    "# Returns:\n",
    "#   - None\n",
    "########################################################################\n",
    "def print_trace(trace):\n",
    "    print (\"[\", datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\"), \"]:\", trace)\n",
    "    \n",
    "\n",
    "########################################################################\n",
    "# get_assay_file_name: Constructs an assay file name from a prefix and assay codes.\n",
    "#\n",
    "# Inputs:\n",
    "#   - prefix: Optional string prefix for the file name.\n",
    "#   - assay_codes: List of assay codes to include in the file name.\n",
    "#\n",
    "# Returns:\n",
    "#   - Generated assay file name as a string.\n",
    "########################################################################\n",
    "def get_assay_file_name (prefix, assay_codes):\n",
    "    file_name = \"\"  \n",
    "    \n",
    "    if (prefix is not None):\n",
    "        file_name = file_name + prefix\n",
    "    \n",
    "    first_added = False\n",
    "    for assay_code in assay_codes:\n",
    "        if (first_added):\n",
    "            file_name = file_name + \"#\"\n",
    "            \n",
    "        file_name = file_name + str(assay_code)\n",
    "        \n",
    "        first_added = True\n",
    "        \n",
    "    return file_name\n",
    "\n",
    "########################################################################\n",
    "#  get_assay_folder_dataset_name: Generates the full path for an assay \n",
    "# dataset folder.\n",
    "#\n",
    "# Inputs:\n",
    "#   - dest_folder: Destination folder path.\n",
    "#   - prefix: Optional string prefix.\n",
    "#   - assay_codes: List of assay codes.\n",
    "#\n",
    "# Returns:\n",
    "#   - Complete dataset folder path as a string.\n",
    "########################################################################\n",
    "def get_assay_folder_dataset_name (dest_folder, prefix, assay_codes):\n",
    "    return dest_folder + \"/\" + get_assay_file_name(prefix, assay_codes)\n",
    "\n",
    "########################################################################\n",
    "#  get_assay_activity_file_name: Constructs full CSV file path for \n",
    "# assay activities.\n",
    "#\n",
    "# Inputs:\n",
    "#   - dest_folder: Destination folder path.\n",
    "#   - prefix: Optional prefix for naming.\n",
    "#   - assay_codes: List of assay codes.\n",
    "#\n",
    "# Returns:\n",
    "#   - Complete CSV file path as a string.\n",
    "########################################################################\n",
    "def get_assay_activity_file_name (dest_folder, prefix, assay_codes):\n",
    "    return get_assay_folder_dataset_name(dest_folder, prefix, assay_codes) + \".csv\"   \n",
    "\n",
    "########################################################################\n",
    "# get_assay_model_file_name: Constructs full model file path (.h5).\n",
    "#\n",
    "# Inputs:\n",
    "#   - dest_folder: Destination folder path.\n",
    "#   - prefix: Optional prefix for naming.\n",
    "#   - assay_codes: List of assay codes.\n",
    "#\n",
    "# Returns:\n",
    "#   - Complete model file path as a string.\n",
    "########################################################################\n",
    "def get_assay_model_file_name (dest_folder, prefix, assay_codes):\n",
    "    return get_assay_folder_dataset_name(dest_folder, prefix, assay_codes) + \".h5\"   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################\n",
    "#  get_data_from_file_by_act: Obtencion de datos a partir de los \n",
    "# ficheros que se ecuenten en la carpeta input_path\n",
    "#  Los datos obtenido se catalogarán por la actividad marcada por el \n",
    "# parametro activity\n",
    "#   Inputs:\n",
    "#    - input_path: Path donde se encuentran los dicheros con los raw data\n",
    "#    - activity: Matriz de actividades de los raw data pasados\n",
    "#    - X: Matriz de entrada de entrenamiento (formatted raw data)\n",
    "#    - y: Matriz de salida de entrenamiento (formatted activities)\n",
    "#    - fromX: Desplazamiento en el eje X aplicable a la lectura de los raw data\n",
    "#    - fromY: Desplazamiento sobre el eje Y aplicable a la lectura de los raw data\n",
    "#    - numCols: Numero de columnas a leer a partir de fromX\n",
    "#    - numRows: Numero de filas a leer a partir de fromY\n",
    "######################################################################################\n",
    "def get_data_from_file_by_act(input_path, activity, X, y, fromX, fromY, numCols, numRows, prefix = None):\n",
    "    map_rows = [i+fromY for i in range (numRows)]\n",
    "    map_columns = [i+fromX for i in range (numCols)]\n",
    "\n",
    "    for e,i in enumerate(os.listdir(input_path)):\n",
    "        if (prefix is None or i.startswith(prefix)):\n",
    "            if (use_cache and i in file_cache.keys()):\n",
    "                x_scaled = file_cache[i]\n",
    "            else:\n",
    "                #Leemos el dato desde el fichero\n",
    "                df = pd.read_csv(input_path + '/' + i,skiprows = 2)\n",
    "\n",
    "                #Realizamos la normalizaicón para esta muestra\n",
    "                min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "                x_scaled = min_max_scaler.fit_transform(df.values)\n",
    "                x_scaled = np.expand_dims(x_scaled, axis=2)\n",
    "\n",
    "                if (use_cache):\n",
    "                    file_cache[i] = x_scaled\n",
    "\n",
    "            # Si hay datos en el reango de ventana seleccinado, incorporamos la lectura en la matriz X e y\n",
    "            if (x_scaled.shape[1] >= fromX+numCols and x_scaled.shape[0] >= fromY+numRows):\n",
    "                x_scaled_cut = x_scaled[np.ix_(map_rows, map_columns)]\n",
    "                X += [x_scaled_cut]\n",
    "                y += [activity]\n",
    "            #else:\n",
    "            #    print (\"x_scaled.shape[1] \", x_scaled.shape[1])\n",
    "            #    print (\"fromX+numCols \", fromX+numCols)\n",
    "            #    print (\"x_scaled.shape[0] \", x_scaled.shape[0])\n",
    "            #    print (\"fromY+numRows \", fromY+numRows)\n",
    "     \n",
    "\n",
    "######################################################################################\n",
    "#  get_data_from_file: Obtencion de datos a partir de los ficheros que se ecuenten en \n",
    "# la carpeta input_path para activos e inactivos\n",
    "#   Inputs:\n",
    "#    - input_path: Path donde se encuentran los dicheros con los raw data\n",
    "#    - fromX: Desplazamiento en el eje X aplicable a la lectura de los raw data\n",
    "#    - fromY: Desplazamiento sobre el eje Y aplicable a la lectura de los raw data\n",
    "#    - numCols: Numero de columnas a leer a partir de fromX\n",
    "#    - numRows: Numero de filas a leer a partir de fromY\n",
    "#  Return:\n",
    "#    - X vector: Matriz de entrada del entrenamiento\n",
    "#    - y vector: Vector de salida del entrenamiento\n",
    "######################################################################################\n",
    "def get_data_from_file(input_path, fromX, fromY, numCols, numRows, prefix = None):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    get_data_from_file_by_act (input_path + '/' + sub_folder_act, 1, X, y, fromX, fromY, numCols, numRows, prefix)\n",
    "    get_data_from_file_by_act (input_path + '/' + sub_folder_inact, 0, X, y, fromX, fromY, numCols, numRows, prefix)\n",
    "            \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "######################################################################################\n",
    "#  Obtencion de datos a partir de los ficheros que se ecuenten en \n",
    "# la carpeta de training, validation y test\n",
    "#   Inputs:\n",
    "#    - fromX: Desplazamiento en el eje X aplicable a la lectura de los raw data\n",
    "#    - fromY: Desplazamiento sobre el eje Y aplicable a la lectura de los raw data\n",
    "#    - numCols: Numero de columnas a leer a partir de fromX\n",
    "#    - numRows: Numero de filas a leer a partir de fromY\n",
    "#    - custom_verbose: 0 = Sin trazas  1 = Con trazas\n",
    "#  Return:\n",
    "#    - x_train: Matriz de entrada del entrenamiento\n",
    "#    - y_train: Vector de salida del entrenamiento\n",
    "#    - x_val: Matriz de entrada de validacion\n",
    "#    - y_val: Vector de salida de validacion\n",
    "#    - x_test: Matriz de entrada de la fase de tests\n",
    "#    - y_test: Vector de salida de la fase de tests\n",
    "######################################################################################\n",
    "def get_data_from_files(dataset_folder, fromX, fromY, numCols, numRows, custom_verbose = 1, prefix = None):\n",
    "    if (custom_verbose > 0):\n",
    "        print_trace(\"Loading \" + dataset_folder + \"/\" + sub_folder_train)\n",
    "    x_train, y_train = get_data_from_file(dataset_folder + \"/\" + sub_folder_train, fromX, fromY, numCols, numRows, prefix)\n",
    "    \n",
    "    if (custom_verbose > 0):\n",
    "        print_trace (\"Loading \" + dataset_folder + \"/\" + sub_folder_val)\n",
    "    x_val, y_val = get_data_from_file(dataset_folder + \"/\" + sub_folder_val, fromX, fromY, numCols, numRows, prefix)\n",
    "    \n",
    "    if (custom_verbose > 0):\n",
    "        print_trace (\"Loading \" + dataset_folder + \"/\" + sub_folder_test)\n",
    "    x_test, y_test = get_data_from_file(dataset_folder + \"/\" + sub_folder_test, fromX, fromY, numCols, numRows, prefix)\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# precision: Computes the precision metric.\n",
    "#\n",
    "# Inputs:\n",
    "#   - y_true: True labels tensor.\n",
    "#   - y_pred: Predicted labels tensor.\n",
    "#\n",
    "# Returns:\n",
    "#   - Precision score as a float tensor.\n",
    "########################################################################\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
    "    \n",
    "    return precision_keras\n",
    "\n",
    "########################################################################\n",
    "# recall: Computes the recall metric.\n",
    "#\n",
    "# Inputs:\n",
    "#   - y_true: True labels tensor.\n",
    "#   - y_pred: Predicted labels tensor.\n",
    "#\n",
    "# Returns:\n",
    "#   - Recall score as a float tensor.\n",
    "########################################################################\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall_keras\n",
    "\n",
    "########################################################################\n",
    "# f1: Computes the F1-score metric.\n",
    "#\n",
    "# Inputs:\n",
    "#   - y_true: True labels tensor.\n",
    "#   - y_pred: Predicted labels tensor.\n",
    "#\n",
    "# Returns:\n",
    "#   - F1-score as a float tensor.\n",
    "########################################################################\n",
    "def f1(y_true, y_pred):\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    return 2 * ((p * r) / (p + r + K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# create_model: Creación del modelo de entrenamiento\n",
    "#   Inputs:\n",
    "#    - numCols: Numero de columnas de los datos de entrada\n",
    "#    - numRows: Numero de filas de los datos de entrada\n",
    "#  Return:\n",
    "#    - model: Modelo a entrenar\n",
    "########################################################################\n",
    "def create_model (numCols\n",
    "                , numRows\n",
    "                , custom_verbose = 0\n",
    "                , p_metrix_list=['acc'\n",
    "                               , precision\n",
    "                               , recall\n",
    "                               , f1                        \n",
    "                          ]\n",
    "                , p_conv_extr = [[16, 3], [32, 3], [64,3]]\n",
    "                , p_dense = [[512, 0.3], [128, 0.2]]\n",
    "                   ):\n",
    "\n",
    "    # Nuestro feature map será de numRowsxnumCols\n",
    "    img_input = layers.Input(shape=(numRows, numCols, 1))\n",
    "\n",
    "    x = img_input\n",
    "    for conv_extr in p_conv_extr:        \n",
    "        print_trace(\"x = layers.Conv2D({0}, {1}, activation='relu')(img_input) \".format(conv_extr[0], conv_extr[1]))    \n",
    "        x = layers.Conv2D(conv_extr[0], conv_extr[1], activation='relu')(x)\n",
    "        x = layers.MaxPooling2D(2)(x)\n",
    "\n",
    "    # Flatten feature map to a 1-dim tensor\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    for dense in p_dense:        \n",
    "        # Create a fully connected layer with ReLU activation and N hidden units\n",
    "        print_trace(\"x = layers.Dense({0}, activation='relu')(x) \".format(dense[0]))    \n",
    "        x = layers.Dense(dense[0], activation='relu')(x)\n",
    "\n",
    "        if (dense[1] > 0):\n",
    "            # Add a dropout rate\n",
    "            print_trace(\"x = layers.Dropout({0})(x) \".format(dense[1]))    \n",
    "            x = layers.Dropout(dense[1])(x)\n",
    "        \n",
    "    \n",
    "    # Create output layer with a single node and sigmoid activation\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "        \n",
    "    # Configure and compile the model\n",
    "    model = Model(img_input, output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=RMSprop(lr=0.001),\n",
    "                  metrics=p_metrix_list\n",
    "                  )\n",
    "        \n",
    "    if (custom_verbose == 1):\n",
    "        model.summary()        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# fit_model: Trains the provided model based on the training data.\n",
    "# Inputs:\n",
    "#  - model: Training model\n",
    "#  - x_train: Input matrix for training\n",
    "#  - y_train: Output vector for training\n",
    "#  - x_val: Input matrix for validation\n",
    "#  - y_val: Output vector for validation\n",
    "#  - x_test: Input matrix for testing phase\n",
    "#  - y_test: Output vector for testing phase\n",
    "#  - custom_verbose: 0 = No logs, 1 = With logs\n",
    "#  - p_epochs: Number of epochs for training (default = 200)\n",
    "# Return:\n",
    "#  - history: Historical data from the training process\n",
    "#  - score: Results of training evaluation score\n",
    "########################################################################\n",
    "def fit_model (model, x_train, y_train, x_val, y_val, x_test, y_test, custom_verbose = 0, p_epochs = 200):\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    batch_size=default_batch_size,\n",
    "                    epochs=p_epochs,\n",
    "                    verbose=custom_verbose,\n",
    "                    validation_data=(x_val, y_val))\n",
    "    score = model.evaluate(x_test, y_test, verbose=custom_verbose)\n",
    "    \n",
    "    return history, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# save_history_logs: Saves training history data logs into CSV files.\n",
    "#\n",
    "# Inputs:\n",
    "#   - history: Training history object from model training.\n",
    "#   - assay_item_name_full_path: Path and base name for saving logs.\n",
    "#   - graph_list: List of metrics to save (default: ['acc', 'precision', 'recall', 'f1']).\n",
    "#\n",
    "# Returns:\n",
    "#   - None\n",
    "########################################################################\n",
    "def save_history_logs (history\n",
    "                         , assay_item_name_full_path\n",
    "                         , graph_list = ['acc', 'precision', 'recall', 'f1']):\n",
    "    \n",
    "    for graph in graph_list:         \n",
    "        if (not history.history[graph] is None):\n",
    "            with open(assay_item_name_full_path + \"##hist_{}.log\".format(graph), mode='w', newline='') as csv_file:\n",
    "                writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                writer.writerow(history.history[graph])\n",
    "        \n",
    "########################################################################\n",
    "# save_history_pictures: Saves plots of training and validation metrics.\n",
    "#\n",
    "# Inputs:\n",
    "#   - history: Training history object from model training.\n",
    "#   - assay_item_name_full_path: Path and base name for saving plots.\n",
    "#   - graph_list: List of metrics to plot (default: ['acc', 'precision', 'recall', 'f1']).\n",
    "#\n",
    "# Returns:\n",
    "#   - None\n",
    "########################################################################\n",
    "def save_history_pictures (history\n",
    "                         , assay_item_name_full_path\n",
    "                         , graph_list = ['acc', 'precision', 'recall', 'f1']):\n",
    "    \n",
    "    for graph in graph_list:         \n",
    "        if (not history.history[graph] is None):\n",
    "            plt.clf()\n",
    "            plt.plot(history.history[graph])\n",
    "            plt.plot(history.history['val_{}'.format(graph)])\n",
    "            plt.title('model {}'.format(graph))\n",
    "            plt.ylabel(graph)\n",
    "            plt.xlabel('epoch')\n",
    "            plt.legend(['train', 'validation'], loc='lower right')\n",
    "            plt.savefig(assay_item_name_full_path + \"_{}.jpg\".format(graph))\n",
    "   \n",
    "\n",
    "########################################################################\n",
    "# create_global_overview_file: Creates and initializes a global overview file.\n",
    "#\n",
    "# Inputs:\n",
    "#   - data_model_folder: Folder path where the file will be saved.\n",
    "#   - global_overview_file_name: Name of the overview file (default predefined).\n",
    "#\n",
    "# Returns:\n",
    "#   - None\n",
    "########################################################################\n",
    "def create_global_overview_file (data_model_folder, global_overview_file_name = default_global_overview_file_name):\n",
    "    if not os.path.exists(data_model_folder):\n",
    "        os.makedirs(data_model_folder)\n",
    "    \n",
    "    with open(data_model_folder + \"/\" + global_overview_file_name, mode='w', newline='') as log_file:\n",
    "        log_file.write(\"\\\"name\\\",\\\"Gobal ratio\\\",\\\"Negative precision\\\",\\\"Positive precision\\\",\\\"Precision\\\",\\\"Recall\\\",\\\"F1\\\",\\\"True Negatives\\\",\\\"False Positives\\\",\\\"False Negatives\\\",\\\"True Positives\\\",\\\"total elements\\\",\\\"te_pos\\\",\\\"te_neg\\\",\\\"Positive recall\\\",\\\"Negative recall\\\"\\n\") \n",
    "\n",
    "        \n",
    "########################################################################\n",
    "# save_score_traces: Saves model scoring metrics and confusion matrix.\n",
    "#\n",
    "# Inputs:\n",
    "#   - model: Trained model object.\n",
    "#   - score: Evaluation scores from testing.\n",
    "#   - x_test: Test dataset input matrix.\n",
    "#   - y_test: Test dataset output vector.\n",
    "#   - data_model_folder: Folder to store score files.\n",
    "#   - assay_item_name: Identifier for saving the results.\n",
    "#   - global_overview_file_name: Overview filename (default predefined).\n",
    "#\n",
    "# Returns:\n",
    "#   - None\n",
    "######################################################################## \n",
    "def save_score_traces (model, score, x_test, y_test, data_model_folder, assay_item_name, global_overview_file_name = default_global_overview_file_name):\n",
    "    assay_item_name_full_path = data_model_folder + \"/\" + assay_item_name   \n",
    "    \n",
    "    y_test_predict = model.predict(x_test)\n",
    "    y_test_predict = [1 if y > default_positive_perc_th else 0 for y in y_test_predict]    \n",
    "    cm=confusion_matrix(y_test,y_test_predict)\n",
    "    \n",
    "    negative_ratio = (cm[0][0]) / (cm[0][0] +  cm[1][0])\n",
    "    \n",
    "    positive_ratio = (cm[1][1]) / (cm[0][1] +  cm[1][1])\n",
    "    \n",
    "    with open(assay_item_name_full_path + \"##global.log\", mode='w', newline='') as log_file:\n",
    "        log_file.write(\"Gobal ratio: \" + str(score[1]) + \"\\n\") \n",
    "        log_file.write(\"Negative ratio: \" +  str(negative_ratio) + \"\\n\") \n",
    "        log_file.write(\"Positive ratio: \" +  str(positive_ratio) + \"\\n\") \n",
    "        log_file.write(str(score) + \"\\n\") \n",
    "        log_file.write(str(cm) + \"\\n\")                 \n",
    "    \n",
    "    with open(data_model_folder + \"/\" + global_overview_file_name, mode='a', newline='') as log_file:\n",
    "        log_file.write(assay_item_name)\n",
    "        log_file.write(\",\")\n",
    "        log_file.write(str(score[1]))\n",
    "        log_file.write(\",\")\n",
    "        log_file.write(str(negative_ratio))\n",
    "        log_file.write(\",\")\n",
    "        log_file.write(str(positive_ratio))\n",
    "        log_file.write(\",\")\n",
    "        \n",
    "        log_file.write(str(score[2]))\n",
    "        log_file.write(\",\")\n",
    "        log_file.write(str(score[3]))\n",
    "        log_file.write(\",\")\n",
    "        log_file.write(str(score[4]))\n",
    "        log_file.write(\",\")\n",
    "\n",
    "        log_file.write(str(cm[0][0]))\n",
    "        log_file.write(\",\")\n",
    "        log_file.write(str(cm[0][1]))\n",
    "        log_file.write(\",\")\n",
    "        log_file.write(str(cm[1][0]))\n",
    "        log_file.write(\",\")\n",
    "        log_file.write(str(cm[1][1]))\n",
    "        log_file.write(\",\")\n",
    "        \n",
    "        log_file.write(str(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1]))\n",
    "        log_file.write(\",\")\n",
    "        \n",
    "        total_pos = cm[1][0]+cm[1][1]\n",
    "        total_neg = cm[0][0]+cm[0][1]\n",
    "        log_file.write(str(total_pos))\n",
    "        log_file.write(\",\")\n",
    "        log_file.write(str(total_neg))\n",
    "        log_file.write(\",\")\n",
    "\n",
    "        log_file.write(str(cm[1][1]/total_pos))\n",
    "        log_file.write(\",\")\n",
    "        log_file.write(str(cm[0][0]/total_neg))\n",
    "\n",
    "        log_file.write(\"\\n\")\n",
    "    \n",
    "########################################################################\n",
    "# train_assay_model: Manages the full training pipeline including dataset\n",
    "# loading, model training, evaluation, and saving results.\n",
    "#\n",
    "# Inputs:\n",
    "#   - dataset_folder: Root folder for datasets.\n",
    "#   - assay_codes: Specific assay identifiers.\n",
    "#   - data_model_folder: Directory to save trained model and logs.\n",
    "#   - dataset: Pre-loaded datasets as (x_train, y_train, x_val, y_val, x_test, y_test).\n",
    "#   - custom_verbose: Verbosity control (0 = silent, 1 = detailed logs).\n",
    "#   - prefix: Optional dataset naming prefix.\n",
    "#   - p_epochs: Training epochs (default predefined).\n",
    "#   - p_metrix_list: List of metrics to evaluate.\n",
    "#   - p_assay_item_name: Optional override name for the assay item.\n",
    "#   - p_conv_extr: Convolutional layers configuration.\n",
    "#   - p_dense: Dense layers configuration.\n",
    "#\n",
    "# Returns:\n",
    "#   - None\n",
    "########################################################################\n",
    "def train_assay_model (dataset_folder\n",
    "                     , assay_codes\n",
    "                     , data_model_folder\n",
    "                     , dataset\n",
    "                     , custom_verbose = 0\n",
    "                     , prefix = None\n",
    "                     , p_epochs = default_data_training_num_epocs\n",
    "                     , p_metrix_list=['acc'\n",
    "                               , precision\n",
    "                               , recall\n",
    "                               , f1                        \n",
    "                          ]\n",
    "                      , p_assay_item_name = None \n",
    "                      , p_conv_extr = [[16, 3], [32, 3], [64,3]]\n",
    "                      , p_dense = [[512, 0.3], [128, 0.2]]                       \n",
    "                      ):\n",
    "    \n",
    "    assay_item_name = \"\"\n",
    "    if (p_assay_item_name is None):\n",
    "        #assay_item_name_full_path = get_assay_folder_dataset_name(data_model_folder, prefix_model_file, assay_codes)  \n",
    "        assay_item_name = get_assay_file_name(prefix_model_file, assay_codes)\n",
    "    else:\n",
    "        #assay_item_name_full_path = data_model_folder + \"/\" + p_assay_item_name   \n",
    "        assay_item_name = p_assay_item_name\n",
    "    \n",
    "    assay_item_name_full_path = data_model_folder + \"/\" + p_assay_item_name   \n",
    "    \n",
    "    gc.collect()\n",
    "    K.clear_session()    \n",
    "    \n",
    "    print_trace(\"Training dataset for assays \" + str(assay_codes))    \n",
    "    \n",
    "    #Creates the model\n",
    "    print_trace(\"Creating model\")\n",
    "    model = create_model(dataRegionEspNumColumns\n",
    "                       , dataRegionEspNumRows\n",
    "                       , p_metrix_list = p_metrix_list\n",
    "                       , p_conv_extr = p_conv_extr\n",
    "                       , p_dense = p_dense)\n",
    "    \n",
    "    #Getting datasets\n",
    "    dataset_folder = get_assay_folder_dataset_name(dataset_folder, prefix_dataset_folder, assay_codes)    \n",
    "    print_trace(\"Getting datasets from \" + dataset_folder)\n",
    "    x_train = dataset[0]\n",
    "    y_train = dataset[1]\n",
    "    x_val = dataset[2]\n",
    "    y_val = dataset[3]\n",
    "    x_test = dataset[4]\n",
    "    y_test = dataset[5]\n",
    "    #x_train, y_train, x_val, y_val, x_test, y_test = get_data_from_files (dataset_folder, dataRegionEspFromX, dataRegionEspFromY, dataRegionEspNumColumns, dataRegionEspNumRows, custom_verbose, prefix)\n",
    "    \n",
    "    #Fit the model\n",
    "    print_trace(\"Fitting the model for \" + str(x_train.shape))\n",
    "    history, score = fit_model (model\n",
    "                              , x_train\n",
    "                              , y_train\n",
    "                              , x_val\n",
    "                              , y_val\n",
    "                              , x_test\n",
    "                              , y_test\n",
    "                              , custom_verbose\n",
    "                              , p_epochs\n",
    "                              )\n",
    "        \n",
    "    #Saving the model\n",
    "    destination_model = assay_item_name_full_path + '.h5'\n",
    "    print_trace(\"Saving model in  \" + destination_model)\n",
    "    model.save(destination_model)\n",
    "\n",
    "    #Saving traiing traces\n",
    "    save_history_pictures (history, assay_item_name_full_path)\n",
    "    save_history_logs (history, assay_item_name_full_path)\n",
    "    save_score_traces (model, score, x_test, y_test, data_model_folder, assay_item_name)    \n",
    "    \n",
    "    print_trace(\"Model creation finished\")  \n",
    "    \n",
    "    print_trace(\"Clear keras session\")      \n",
    "    K.clear_session()    \n",
    "    \n",
    "    print_trace(\"Purge memory for model\")      \n",
    "    del model\n",
    "    \n",
    "    print_trace(\"Purge memory for x_train\")  \n",
    "    #del x_train\n",
    "    \n",
    "    print_trace(\"Purge memory for y_train\")  \n",
    "    #del y_train\n",
    "    \n",
    "    print_trace(\"Purge memory for x_val\")  \n",
    "    #del x_val\n",
    "    \n",
    "    print_trace(\"Purge memory for y_val\")  \n",
    "    #del y_val\n",
    "    \n",
    "    print_trace(\"Purge memory for x_test\")  \n",
    "    #del x_test\n",
    "    \n",
    "    print_trace(\"Purge memory for y_test\")  \n",
    "    #del y_test    \n",
    "    \n",
    "    print_trace(\"Recollect free memory\")      \n",
    "    gc.collect()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of image_classification_part2.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/google/eng-edu/blob/master/ml/pc/exercises/image_classification_part2.ipynb",
     "timestamp": 1590165853129
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
